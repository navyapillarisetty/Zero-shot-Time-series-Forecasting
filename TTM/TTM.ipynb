{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMuTVsSc2zKA"
      },
      "outputs": [],
      "source": [
        "! pip install \"granite-tsfm[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.22\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments,set_seed\n",
        "from transformers.integrations import INTEGRATION_TO_CALLBACK\n",
        "\n",
        "from tsfm_public import TimeSeriesPreprocessor, TrackingCallback, count_parameters, get_datasets\n",
        "from tsfm_public.toolkit.get_model import get_model\n",
        "from tsfm_public.toolkit.lr_finder import optimal_lr_finder\n",
        "from tsfm_public.toolkit.visualization import plot_predictions"
      ],
      "metadata": {
        "id": "MJIDabE64Hwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "N7MXt0TD4Xgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "#SEED = 45\n",
        "#set_seed(SEED)\n",
        "import time\n",
        "\n",
        "# TTM Model path. The default model path is Granite-R2. Below, you can choose other TTM releases.\n",
        "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
        "# TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r1\"\n",
        "# TTM_MODEL_PATH = \"ibm-research/ttm-research-r2\"\n",
        "\n",
        "# Context length, Or Length of the history.\n",
        "# Currently supported values are: 512/1024/1536 for Granite-TTM-R2 and Research-Use-TTM-R2, and 512/1024 for Granite-TTM-R1\n",
        "CONTEXT_LENGTH = 90\n",
        "\n",
        "# Granite-TTM-R2 supports forecast length upto 720 and Granite-TTM-R1 supports forecast length upto 96\n",
        "PREDICTION_LENGTH = 24\n",
        "\n",
        "# Results dir\n",
        "OUT_DIR = \"../results/\""
      ],
      "metadata": {
        "id": "a4vhOGio4nLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path= \"../L1MAG.csv.bin\"\n",
        "dataset= np.fromfile(dataset_path)\n",
        "dataset_path2= \"../L1MAG_part2_summary_statistics.csv\"\n",
        "\n",
        "if data_path2.endswith(\".csv\"):\n",
        "  hint_data= np.loadtxt(data_path2)\n",
        "elif data_path2.endswith(\".bin\"):\n",
        "  hint_data= np.fromfile(data_path2)\n",
        "\n",
        "new_data1= np.concatenate((dataset,hint_data))\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "new_data= new_data1.reshape(-1,1)\n",
        "fit_data= scaler.fit(new_data)\n",
        "new_data= scaler.transform(new_data)\n",
        "\n",
        "#print(new_data)\n",
        "timestamp_column = \"date\"\n",
        "\n",
        "date_new= pd.date_range(start=pd.to_datetime(\"2018-07-01\"), periods=len(new_data), freq=\"H\")\n",
        "new_data= new_data.flatten()\n",
        "df_raw= pd.DataFrame({\"date\":date_new,\"Data\":new_data})\n",
        "print(df_raw)\n",
        "\n",
        "id_columns = []  # mention the ids that uniquely identify a time-series.\n",
        "\n",
        "target_columns = [\"Data\"]\n",
        "split_config = {\n",
        "    \"train\": [0, 3668],\n",
        "    \"valid\": [3668,3767],\n",
        "    \"test\": [\n",
        "        3767,\n",
        "        7535,\n",
        "    ],\n",
        "}\n",
        "\n",
        "column_specifiers = {\n",
        "    \"timestamp_column\": timestamp_column,\n",
        "    \"id_columns\": id_columns,\n",
        "    \"target_columns\": target_columns,\n",
        "    \"control_columns\": [],\n",
        "}"
      ],
      "metadata": {
        "id": "192g6Wg74yHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name= \"Hinted_dataset\""
      ],
      "metadata": {
        "id": "SgRzd0G2-x6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test= 0\n",
        "pred= 0\n",
        "def fewshot_finetune_eval(\n",
        "    dataset_name,\n",
        "    batch_size,\n",
        "    learning_rate=None,\n",
        "    context_length= CONTEXT_LENGTH,\n",
        "    forecast_length= PREDICTION_LENGTH,\n",
        "    fewshot_percent=100,\n",
        "    freeze_backbone=True,\n",
        "    num_epochs=50,\n",
        "    save_dir=OUT_DIR,\n",
        "    loss=\"mse\",\n",
        "    quantile=0.5,\n",
        "):\n",
        "    out_dir = os.path.join(save_dir, dataset_name)\n",
        "\n",
        "    # Data prep: Get dataset\n",
        "\n",
        "    tsp = TimeSeriesPreprocessor(\n",
        "        **column_specifiers,\n",
        "        context_length=context_length,\n",
        "        prediction_length=forecast_length,\n",
        "        scaling=False,\n",
        "        encode_categorical=False,\n",
        "        scaler_type=\"standard\",\n",
        "    )\n",
        "\n",
        "    finetune_forecast_model = get_model(\n",
        "        TTM_MODEL_PATH,\n",
        "        context_length=context_length,\n",
        "        prediction_length=forecast_length,\n",
        "        freq_prefix_tuning=False,\n",
        "        freq=None,\n",
        "        force_return= \"random_init_medium\",\n",
        "        prefer_l1_loss=False,\n",
        "        prefer_longer_context=True,\n",
        "        # Can also provide TTM Config args\n",
        "        loss=loss,\n",
        "        quantile=quantile,\n",
        "    )\n",
        "\n",
        "    dset_train, dset_val, dset_test = get_datasets(\n",
        "        tsp,\n",
        "        df_raw,\n",
        "        split_config,\n",
        "        fewshot_fraction=fewshot_percent / 100,\n",
        "        fewshot_location=\"first\",\n",
        "        use_frequency_token=finetune_forecast_model.config.resolution_prefix_tuning,\n",
        "    )\n",
        "\n",
        "    if freeze_backbone:\n",
        "        print(\n",
        "            \"Number of params before freezing backbone\",\n",
        "            count_parameters(finetune_forecast_model),\n",
        "        )\n",
        "\n",
        "        # Freeze the backbone of the model\n",
        "        for param in finetune_forecast_model.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Count params\n",
        "        print(\n",
        "            \"Number of params after freezing the backbone\",\n",
        "            count_parameters(finetune_forecast_model),\n",
        "        )\n",
        "\n",
        "    # Find optimal learning rate\n",
        "    # Use with caution: Set it manually if the suggested learning rate is not suitable\n",
        "    if learning_rate is None:\n",
        "        learning_rate, finetune_forecast_model = optimal_lr_finder(\n",
        "            finetune_forecast_model,\n",
        "            dset_train,\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        print(\"OPTIMAL SUGGESTED LEARNING RATE =\", learning_rate)\n",
        "\n",
        "    print(f\"Using learning rate = {learning_rate}\")\n",
        "    finetune_forecast_args = TrainingArguments(\n",
        "        output_dir=os.path.join(out_dir, \"output\"),\n",
        "        overwrite_output_dir=True,\n",
        "        learning_rate=learning_rate,\n",
        "        num_train_epochs=num_epochs,\n",
        "        do_eval=True,\n",
        "        eval_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        dataloader_num_workers=8,\n",
        "        report_to=\"none\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        logging_dir=os.path.join(out_dir, \"logs\"),  # Make sure to specify a logging directory\n",
        "        load_best_model_at_end=True,  # Load the best model when training ends\n",
        "        metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
        "        greater_is_better=False,  # For loss\n",
        "        seed=int(time.time()),\n",
        "    )\n",
        "\n",
        "    # Create the early stopping callback\n",
        "    early_stopping_callback = EarlyStoppingCallback(\n",
        "        early_stopping_patience=10,  # Number of epochs with no improvement after which to stop\n",
        "        early_stopping_threshold=1e-5,  # Minimum improvement required to consider as improvement\n",
        "    )\n",
        "    tracking_callback = TrackingCallback()\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = AdamW(finetune_forecast_model.parameters(), lr=learning_rate)\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        learning_rate,\n",
        "        epochs=num_epochs,\n",
        "        steps_per_epoch=math.ceil(len(dset_train) / (batch_size)),\n",
        "    )\n",
        "\n",
        "    finetune_forecast_trainer = Trainer(\n",
        "        model=finetune_forecast_model,\n",
        "        args=finetune_forecast_args,\n",
        "        train_dataset=dset_train,\n",
        "        eval_dataset=dset_val,\n",
        "        callbacks=[early_stopping_callback, tracking_callback],\n",
        "        optimizers=(optimizer, scheduler),\n",
        "    )\n",
        "    finetune_forecast_trainer.remove_callback(INTEGRATION_TO_CALLBACK[\"codecarbon\"])\n",
        "\n",
        "    # Fine tune\n",
        "    finetune_forecast_trainer.train()\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"+\" * 20, f\"Test MSE after few-shot {fewshot_percent}% fine-tuning\", \"+\" * 20)\n",
        "\n",
        "    finetune_forecast_trainer.model.loss = \"mse\"  # fixing metric to mse for evaluation\n",
        "\n",
        "    fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n",
        "    print(fewshot_output)\n",
        "    print(\"+\" * 60)\n",
        "    print(dset_test[0])\n",
        "\n",
        "    # get predictions\n",
        "\n",
        "    predictions_dict = finetune_forecast_trainer.predict(dset_test)\n",
        "    #print(\"predictions_dict\",predictions_dict)\n",
        "\n",
        "    predictions_np = predictions_dict.predictions[0]\n",
        "    print(\"predictions_np\",predictions_np)\n",
        "    print(\"testing data\", dset_test[0])\n",
        "    print(predictions_np.shape)\n",
        "    predictions_np= np.array(predictions_np)\n",
        "    pred= predictions_np.flatten()\n",
        "    np.savetxt(\"../results/ttm_pred_data.csv\",pred)\n",
        "\n",
        "    # get backbone embeddings (if needed for further analysis)\n",
        "\n",
        "    backbone_embedding = predictions_dict.predictions[1]\n",
        "\n",
        "    print(backbone_embedding.shape)\n",
        "\n",
        "\n",
        "    # plot\n",
        "    plot_predictions(\n",
        "        model=finetune_forecast_trainer.model,\n",
        "        dset=dset_test,\n",
        "        plot_dir=os.path.join(OUT_DIR, dataset_name),\n",
        "        plot_prefix=\"test_fewshot\",\n",
        "        indices=[685, 118, 902, 1984, 894, 967, 304, 57, 265, 1015],\n",
        "        channel=0,\n",
        "    )"
      ],
      "metadata": {
        "id": "N7CS9Vy--82h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fewshot_finetune_eval(\n",
        "  dataset_name=dataset_name,\n",
        "  context_length= CONTEXT_LENGTH,\n",
        "  forecast_length= PREDICTION_LENGTH,\n",
        "  batch_size=64,\n",
        "  fewshot_percent=100,\n",
        "  earning_rate=0.001,\n",
        "  )"
      ],
      "metadata": {
        "id": "zvw7hjmT_zQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}